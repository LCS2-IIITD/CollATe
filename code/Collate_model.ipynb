{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Embeddings Import\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tf_sentencepiece\n",
    "\n",
    "# System Utils\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from sklearn.metrics import recall_score,accuracy_score\n",
    "\n",
    "# Reading Writing Utils\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "# Scientific Computing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>like_ratio</th>\n",
       "      <th>dislike_ratio</th>\n",
       "      <th>views_to_days</th>\n",
       "      <th>duration</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>peaks_count</th>\n",
       "      <th>heightxwid</th>\n",
       "      <th>label</th>\n",
       "      <th>comments_sim_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0Cu805aSMqY</td>\n",
       "      <td>0.053065</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>12.118110</td>\n",
       "      <td>127.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.084689</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.646691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>WzhoYJk0i-g</td>\n",
       "      <td>0.152995</td>\n",
       "      <td>0.242009</td>\n",
       "      <td>2.411111</td>\n",
       "      <td>181.0</td>\n",
       "      <td>26</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.749086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1           id  like_ratio  dislike_ratio  \\\n",
       "0           8             8  0Cu805aSMqY    0.053065       0.008097   \n",
       "1          44            44  WzhoYJk0i-g    0.152995       0.242009   \n",
       "\n",
       "   views_to_days  duration  comment_count  peaks_count  heightxwid  label  \\\n",
       "0      12.118110     127.0             56          1.0    0.084689    1.0   \n",
       "1       2.411111     181.0             26          2.0    0.011100    1.0   \n",
       "\n",
       "   comments_sim_score  \n",
       "0            0.646691  \n",
       "1            0.749086  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata2=pd.read_csv('/home/himaninegi/final_features_full.csv',sep='\\t')\n",
    "metadata2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = metadata2['label']\n",
    "\n",
    "x=metadata2.drop(columns=['Unnamed: 0','Unnamed: 0.1', 'id', 'label', 'views_to_days'])\n",
    "from sklearn.model_selection import train_test_split\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,stratify=y,test_size=0.25,random_state=42)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, stratify=y_train,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['like_ratio', 'dislike_ratio', 'duration', 'comment_count',\n",
       "       'peaks_count', 'heightxwid', 'comments_sim_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1384, 7), (616, 7), (462, 7))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_test.shape,x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLLATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras import initializers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "\n",
    "class ASL:\n",
    "    \"\"\"\n",
    "        Autoencoder supervised learning class\n",
    "        initialize class with number of labeled and unlabeled\n",
    "        must call cnn_setup() or simple_setup() before any training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_samples_train_labeled, n_samples_train_unlabeled, verbose=0):\n",
    "        self.x_train, self.y_train, self.x_test, self.y_test = x_train, y_train, x_test, y_test\n",
    "\n",
    "        self.n_labeled = n_samples_train_labeled\n",
    "        self.n_unlabeled = n_samples_train_unlabeled\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def simple_setup(self):\n",
    "        \"\"\" data setup for simple single later autoencoder \"\"\"\n",
    "  \n",
    "        self.input_shape = 7\n",
    "        self.num_classes = 2\n",
    "        self.regularized_batch_size = 40\n",
    "        self.basic_batch_size = 25\n",
    "        self.epochs = 150\n",
    "        self.verbose = 1\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        self.y_train = keras.utils.to_categorical(self.y_train, self.num_classes)\n",
    "        self.y_test = keras.utils.to_categorical(self.y_test, self.num_classes)\n",
    "\n",
    "        self.model_creator = self.create_simple_model\n",
    "\n",
    "        # train on n labeled data, rest unlabeled\n",
    "        y_train_pruned = np.copy(self.y_train)\n",
    "        y_train_pruned[self.n_labeled:,:] = 0\n",
    "\n",
    "        self.x_train_labeled = self.x_train.iloc[0:self.n_labeled]\n",
    "        self.y_train_pruned_labeled = y_train_pruned[0:self.n_labeled]\n",
    "        self.y_train_labeled = self.y_train[0:self.n_labeled]\n",
    "\n",
    "        self.x_train_unlabeled = self.x_train.iloc[self.n_labeled: self.n_labeled + self.n_unlabeled]\n",
    "        self.y_train_pruned_unlabeled = y_train_pruned[self.n_labeled: self.n_labeled + self.n_unlabeled]\n",
    "\n",
    "#         random_ordering = np.random.permutation(self.n_labeled + self.n_unlabeled)\n",
    "#         self.x_train_all_shuffled = np.concatenate(\n",
    "#                 (self.x_train_labeled, self.x_train_unlabeled))[random_ordering]\n",
    "#         self.y_train_all_shuffled = np.concatenate(\n",
    "#                 (self.y_train_labeled, self.y_train_pruned_unlabeled))[random_ordering]\n",
    "\n",
    "        self.model_creator = self.create_simple_model\n",
    "        \n",
    "    def create_simple_model(self, regularized): # add a Dense layer with a L1 activity regularizer\n",
    "\n",
    "        visible = Input(shape=(7, ))\n",
    "        encode = Dense(128, activation='relu',\n",
    "                    activity_regularizer=regularizers.l1(10e-5))(visible)\n",
    "        encode = Dense(128, activation='relu',\n",
    "                    activity_regularizer=regularizers.l1(10e-5))(encode)\n",
    "        encode = Dense(5, activation='relu',\n",
    "                    activity_regularizer=regularizers.l1(10e-5))(encode)\n",
    "\n",
    "        output = Dense(self.num_classes, name='class', activation='softmax')(encode)\n",
    "\n",
    "        decode = Dense(128, activation='relu',\n",
    "                    activity_regularizer=regularizers.l1(10e-5))(encode)\n",
    "        decode = Dense(128, activation='relu',\n",
    "                    activity_regularizer=regularizers.l1(10e-5))(decode)\n",
    "        decode = Dense(7, activation='sigmoid',name='reconstruction')(decode)\n",
    "\n",
    "        if regularized:\n",
    "            return Model(inputs=visible, outputs=[output, decode])\n",
    "        else:\n",
    "            return Model(inputs=visible, outputs=output)\n",
    "        \n",
    "    def train_basic_model(self):\n",
    "        \n",
    "        model = self.model_creator(False)\n",
    "        model.summary()\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=keras.optimizers.Adadelta(),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        model.fit(np.copy(self.x_train_labeled),\n",
    "                  np.copy(self.y_train_labeled),\n",
    "                  batch_size=self.basic_batch_size,\n",
    "                  epochs=self.epochs,\n",
    "                  verbose=self.verbose,\n",
    "                  validation_data=(self.x_test, self.y_test))\n",
    "\n",
    "        predictions = model.predict(self.x_test)\n",
    "        #probs = model.predict_proba(self.x_test)\n",
    "        pred_class = [np.argmax(p) for p in predictions]\n",
    "        true_class = [np.argmax(p) for p in self.y_test]\n",
    "\n",
    "        print(\"the length of the true_class is\",len(true_class))\n",
    "        print(\"the lenth of the pred class is\",len(pred_class))\n",
    "        \n",
    "        print(\"the lenth of the pred-prob class is\",len(predictions))\n",
    "        \n",
    "        \n",
    "        print(\"\\n\\n==========================================================\")\n",
    "        print(\"basic model: \" + str(self.n_labeled) + \" samples\")\n",
    "        print(classification_report(true_class, pred_class, digits=4))\n",
    "        \n",
    "        print(\"recall-None\",recall_score(true_class,pred_class,average=None))\n",
    "        print(\"recall-macro \",recall_score(true_class,pred_class,average='macro'))\n",
    "        print(\"recall-micro \",recall_score(true_class,pred_class,average='micro'))\n",
    "        \n",
    "        score = model.evaluate(self.x_test, self.y_test, verbose=0)\n",
    "        print(score)\n",
    "        for metric_name, value in zip(model.metrics_names, score):\n",
    "            print(metric_name + \":\", value)\n",
    "        return true_class,pred_class,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1024      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "class (Dense)                (None, 2)                 12        \n",
      "=================================================================\n",
      "Total params: 18,193\n",
      "Trainable params: 18,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/himaninegi/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1384 samples, validate on 616 samples\n",
      "Epoch 1/150\n",
      "1384/1384 [==============================] - 1s 406us/step - loss: 139.5917 - accuracy: 0.4877 - val_loss: 56.6110 - val_accuracy: 0.4627\n",
      "Epoch 2/150\n",
      "1384/1384 [==============================] - 0s 76us/step - loss: 74.0341 - accuracy: 0.4928 - val_loss: 47.3172 - val_accuracy: 0.5471\n",
      "Epoch 3/150\n",
      "1384/1384 [==============================] - 0s 81us/step - loss: 60.1030 - accuracy: 0.5470 - val_loss: 38.5697 - val_accuracy: 0.5471\n",
      "Epoch 4/150\n",
      "1384/1384 [==============================] - 0s 83us/step - loss: 49.3572 - accuracy: 0.5520 - val_loss: 30.6630 - val_accuracy: 0.5763\n",
      "Epoch 5/150\n",
      "1384/1384 [==============================] - 0s 84us/step - loss: 38.1731 - accuracy: 0.5730 - val_loss: 23.0309 - val_accuracy: 0.5471\n",
      "Epoch 6/150\n",
      "1384/1384 [==============================] - 0s 78us/step - loss: 28.2995 - accuracy: 0.5470 - val_loss: 17.2254 - val_accuracy: 0.5471\n",
      "Epoch 7/150\n",
      "1384/1384 [==============================] - 0s 81us/step - loss: 20.9925 - accuracy: 0.5491 - val_loss: 12.3263 - val_accuracy: 0.5471\n",
      "Epoch 8/150\n",
      "1384/1384 [==============================] - 0s 68us/step - loss: 15.0452 - accuracy: 0.6705 - val_loss: 9.0259 - val_accuracy: 0.6672\n",
      "Epoch 9/150\n",
      "1384/1384 [==============================] - 0s 77us/step - loss: 11.3052 - accuracy: 0.8042 - val_loss: 6.6753 - val_accuracy: 0.8133\n",
      "Epoch 10/150\n",
      "1384/1384 [==============================] - 0s 76us/step - loss: 8.5857 - accuracy: 0.8100 - val_loss: 5.2171 - val_accuracy: 0.8198\n",
      "Epoch 11/150\n",
      "1384/1384 [==============================] - 0s 79us/step - loss: 6.6020 - accuracy: 0.8078 - val_loss: 3.7605 - val_accuracy: 0.8295\n",
      "Epoch 12/150\n",
      "1384/1384 [==============================] - 0s 84us/step - loss: 4.7748 - accuracy: 0.8186 - val_loss: 2.6991 - val_accuracy: 0.8214\n",
      "Epoch 13/150\n",
      "1384/1384 [==============================] - 0s 56us/step - loss: 3.2742 - accuracy: 0.8201 - val_loss: 1.8347 - val_accuracy: 0.8425\n",
      "Epoch 14/150\n",
      "1384/1384 [==============================] - 0s 78us/step - loss: 2.2313 - accuracy: 0.8013 - val_loss: 1.2654 - val_accuracy: 0.8101\n",
      "Epoch 15/150\n",
      "1384/1384 [==============================] - 0s 83us/step - loss: 1.5244 - accuracy: 0.8165 - val_loss: 0.8949 - val_accuracy: 0.8214\n",
      "Epoch 16/150\n",
      "1384/1384 [==============================] - 0s 92us/step - loss: 1.0583 - accuracy: 0.8121 - val_loss: 0.7405 - val_accuracy: 0.7987\n",
      "Epoch 17/150\n",
      "1384/1384 [==============================] - 0s 71us/step - loss: 0.7869 - accuracy: 0.8078 - val_loss: 0.5694 - val_accuracy: 0.8393\n",
      "Epoch 18/150\n",
      "1384/1384 [==============================] - 0s 86us/step - loss: 0.5820 - accuracy: 0.8215 - val_loss: 0.6758 - val_accuracy: 0.6786\n",
      "Epoch 19/150\n",
      "1384/1384 [==============================] - 0s 80us/step - loss: 0.6144 - accuracy: 0.8237 - val_loss: 0.5014 - val_accuracy: 0.8458\n",
      "Epoch 20/150\n",
      "1384/1384 [==============================] - 0s 67us/step - loss: 0.5226 - accuracy: 0.8324 - val_loss: 0.5240 - val_accuracy: 0.8328\n",
      "Epoch 21/150\n",
      "1384/1384 [==============================] - 0s 81us/step - loss: 0.5061 - accuracy: 0.8454 - val_loss: 0.4705 - val_accuracy: 0.8425\n",
      "Epoch 22/150\n",
      "1384/1384 [==============================] - 0s 73us/step - loss: 0.4880 - accuracy: 0.8353 - val_loss: 0.4972 - val_accuracy: 0.8523\n",
      "Epoch 23/150\n",
      "1384/1384 [==============================] - 0s 85us/step - loss: 0.4952 - accuracy: 0.8259 - val_loss: 0.4648 - val_accuracy: 0.8360\n",
      "Epoch 24/150\n",
      "1384/1384 [==============================] - 0s 105us/step - loss: 0.4548 - accuracy: 0.8389 - val_loss: 0.4393 - val_accuracy: 0.8506\n",
      "Epoch 25/150\n",
      "1384/1384 [==============================] - 0s 86us/step - loss: 0.4625 - accuracy: 0.8439 - val_loss: 0.5112 - val_accuracy: 0.8052\n",
      "Epoch 26/150\n",
      "1384/1384 [==============================] - 0s 91us/step - loss: 0.4356 - accuracy: 0.8490 - val_loss: 0.4263 - val_accuracy: 0.8506\n",
      "Epoch 27/150\n",
      "1384/1384 [==============================] - 0s 67us/step - loss: 0.4311 - accuracy: 0.8468 - val_loss: 0.4357 - val_accuracy: 0.8490\n",
      "Epoch 28/150\n",
      "1384/1384 [==============================] - 0s 63us/step - loss: 0.4323 - accuracy: 0.8540 - val_loss: 0.4291 - val_accuracy: 0.8474\n",
      "Epoch 29/150\n",
      "1384/1384 [==============================] - 0s 87us/step - loss: 0.4114 - accuracy: 0.8490 - val_loss: 0.4434 - val_accuracy: 0.8442\n",
      "Epoch 30/150\n",
      "1384/1384 [==============================] - 0s 80us/step - loss: 0.4395 - accuracy: 0.8526 - val_loss: 0.4047 - val_accuracy: 0.8523\n",
      "Epoch 31/150\n",
      "1384/1384 [==============================] - 0s 85us/step - loss: 0.4121 - accuracy: 0.8512 - val_loss: 0.4150 - val_accuracy: 0.8474\n",
      "Epoch 32/150\n",
      "1384/1384 [==============================] - 0s 88us/step - loss: 0.4008 - accuracy: 0.8533 - val_loss: 0.4355 - val_accuracy: 0.8409\n",
      "Epoch 33/150\n",
      "1384/1384 [==============================] - 0s 71us/step - loss: 0.4125 - accuracy: 0.8483 - val_loss: 0.5034 - val_accuracy: 0.8182\n",
      "Epoch 34/150\n",
      "1384/1384 [==============================] - 0s 86us/step - loss: 0.4052 - accuracy: 0.8548 - val_loss: 0.4098 - val_accuracy: 0.8393\n",
      "Epoch 35/150\n",
      "1384/1384 [==============================] - 0s 83us/step - loss: 0.4018 - accuracy: 0.8533 - val_loss: 0.8357 - val_accuracy: 0.6039\n",
      "Epoch 36/150\n",
      "1384/1384 [==============================] - 0s 91us/step - loss: 0.4297 - accuracy: 0.8418 - val_loss: 0.4382 - val_accuracy: 0.8425\n",
      "Epoch 37/150\n",
      "1384/1384 [==============================] - 0s 90us/step - loss: 0.4272 - accuracy: 0.8403 - val_loss: 0.3991 - val_accuracy: 0.8555\n",
      "Epoch 38/150\n",
      "1384/1384 [==============================] - 0s 89us/step - loss: 0.3864 - accuracy: 0.8598 - val_loss: 0.4101 - val_accuracy: 0.8458\n",
      "Epoch 39/150\n",
      "1384/1384 [==============================] - 0s 99us/step - loss: 0.3877 - accuracy: 0.8577 - val_loss: 0.4030 - val_accuracy: 0.8506\n",
      "Epoch 40/150\n",
      "1384/1384 [==============================] - 0s 100us/step - loss: 0.4019 - accuracy: 0.8569 - val_loss: 0.4281 - val_accuracy: 0.8344\n",
      "Epoch 41/150\n",
      "1384/1384 [==============================] - 0s 102us/step - loss: 0.3877 - accuracy: 0.8656 - val_loss: 0.4642 - val_accuracy: 0.8506\n",
      "Epoch 42/150\n",
      "1384/1384 [==============================] - 0s 90us/step - loss: 0.4024 - accuracy: 0.8483 - val_loss: 0.4029 - val_accuracy: 0.8523\n",
      "Epoch 43/150\n",
      "1384/1384 [==============================] - 0s 87us/step - loss: 0.3948 - accuracy: 0.8475 - val_loss: 2.1033 - val_accuracy: 0.5698\n",
      "Epoch 44/150\n",
      "1384/1384 [==============================] - 0s 93us/step - loss: 0.4129 - accuracy: 0.8504 - val_loss: 0.4234 - val_accuracy: 0.8442\n",
      "Epoch 45/150\n",
      "1384/1384 [==============================] - 0s 88us/step - loss: 0.3792 - accuracy: 0.8649 - val_loss: 0.3996 - val_accuracy: 0.8490\n",
      "Epoch 46/150\n",
      "1384/1384 [==============================] - 0s 77us/step - loss: 0.3761 - accuracy: 0.8678 - val_loss: 0.4159 - val_accuracy: 0.8539\n",
      "Epoch 47/150\n",
      "1384/1384 [==============================] - 0s 87us/step - loss: 0.3775 - accuracy: 0.8656 - val_loss: 0.4557 - val_accuracy: 0.8214\n",
      "Epoch 48/150\n",
      "1384/1384 [==============================] - 0s 95us/step - loss: 0.3880 - accuracy: 0.8512 - val_loss: 0.4202 - val_accuracy: 0.8490\n",
      "Epoch 49/150\n",
      "1384/1384 [==============================] - 0s 96us/step - loss: 0.3847 - accuracy: 0.8562 - val_loss: 0.4865 - val_accuracy: 0.8117\n",
      "Epoch 50/150\n",
      "1384/1384 [==============================] - 0s 96us/step - loss: 0.3905 - accuracy: 0.8512 - val_loss: 0.4107 - val_accuracy: 0.8442\n",
      "Epoch 51/150\n",
      "1384/1384 [==============================] - 0s 79us/step - loss: 0.4005 - accuracy: 0.8598 - val_loss: 0.4268 - val_accuracy: 0.8506\n",
      "Epoch 52/150\n",
      "1384/1384 [==============================] - 0s 82us/step - loss: 0.3819 - accuracy: 0.8620 - val_loss: 0.4618 - val_accuracy: 0.8279\n",
      "Epoch 53/150\n",
      "1384/1384 [==============================] - 0s 76us/step - loss: 0.3901 - accuracy: 0.8540 - val_loss: 0.4032 - val_accuracy: 0.8506\n",
      "Epoch 54/150\n",
      "1384/1384 [==============================] - 0s 80us/step - loss: 0.3828 - accuracy: 0.8569 - val_loss: 0.4177 - val_accuracy: 0.8425\n",
      "Epoch 55/150\n",
      "1384/1384 [==============================] - 0s 72us/step - loss: 0.3876 - accuracy: 0.8649 - val_loss: 0.4713 - val_accuracy: 0.8182\n",
      "Epoch 56/150\n",
      "1384/1384 [==============================] - 0s 73us/step - loss: 0.3765 - accuracy: 0.8613 - val_loss: 0.5102 - val_accuracy: 0.7808\n",
      "Epoch 57/150\n",
      "1384/1384 [==============================] - 0s 84us/step - loss: 0.3938 - accuracy: 0.8584 - val_loss: 0.3904 - val_accuracy: 0.8588\n",
      "Epoch 58/150\n",
      "1384/1384 [==============================] - 0s 92us/step - loss: 0.3699 - accuracy: 0.8714 - val_loss: 0.3863 - val_accuracy: 0.8555\n",
      "Epoch 59/150\n",
      "1384/1384 [==============================] - 0s 74us/step - loss: 0.3731 - accuracy: 0.8627 - val_loss: 0.7653 - val_accuracy: 0.7695\n",
      "Epoch 60/150\n",
      "1384/1384 [==============================] - 0s 70us/step - loss: 0.4007 - accuracy: 0.8613 - val_loss: 0.4312 - val_accuracy: 0.8409\n",
      "Epoch 61/150\n",
      "1384/1384 [==============================] - 0s 87us/step - loss: 0.3890 - accuracy: 0.8577 - val_loss: 0.4253 - val_accuracy: 0.8377\n",
      "Epoch 62/150\n",
      "1384/1384 [==============================] - 0s 68us/step - loss: 0.3787 - accuracy: 0.8627 - val_loss: 0.4046 - val_accuracy: 0.8555\n",
      "Epoch 63/150\n",
      "1384/1384 [==============================] - 0s 58us/step - loss: 0.3874 - accuracy: 0.8555 - val_loss: 0.3996 - val_accuracy: 0.8555\n",
      "Epoch 64/150\n",
      "1384/1384 [==============================] - 0s 61us/step - loss: 0.3780 - accuracy: 0.8642 - val_loss: 0.4209 - val_accuracy: 0.8409\n",
      "Epoch 65/150\n",
      "1384/1384 [==============================] - 0s 61us/step - loss: 0.3770 - accuracy: 0.8598 - val_loss: 0.4005 - val_accuracy: 0.8555\n",
      "Epoch 66/150\n",
      "1384/1384 [==============================] - 0s 76us/step - loss: 0.3708 - accuracy: 0.8591 - val_loss: 0.3934 - val_accuracy: 0.8539\n",
      "Epoch 67/150\n",
      "1384/1384 [==============================] - 0s 65us/step - loss: 0.3874 - accuracy: 0.8512 - val_loss: 0.3995 - val_accuracy: 0.8571\n",
      "Epoch 68/150\n",
      "1384/1384 [==============================] - 0s 58us/step - loss: 0.3696 - accuracy: 0.8598 - val_loss: 0.4208 - val_accuracy: 0.8474\n",
      "Epoch 69/150\n",
      "1384/1384 [==============================] - 0s 62us/step - loss: 0.3729 - accuracy: 0.8591 - val_loss: 0.3937 - val_accuracy: 0.8442\n",
      "Epoch 70/150\n",
      "1384/1384 [==============================] - 0s 62us/step - loss: 0.3936 - accuracy: 0.8678 - val_loss: 0.4092 - val_accuracy: 0.8555\n",
      "Epoch 71/150\n",
      "1384/1384 [==============================] - 0s 60us/step - loss: 0.3697 - accuracy: 0.8699 - val_loss: 0.4909 - val_accuracy: 0.8019\n",
      "Epoch 72/150\n",
      "1384/1384 [==============================] - 0s 59us/step - loss: 0.3863 - accuracy: 0.8634 - val_loss: 0.4140 - val_accuracy: 0.8523\n",
      "Epoch 73/150\n",
      "1384/1384 [==============================] - 0s 61us/step - loss: 0.3737 - accuracy: 0.8634 - val_loss: 0.4456 - val_accuracy: 0.8409\n",
      "Epoch 74/150\n",
      "1384/1384 [==============================] - 0s 59us/step - loss: 0.3769 - accuracy: 0.8569 - val_loss: 0.4018 - val_accuracy: 0.8539\n",
      "Epoch 75/150\n",
      "1384/1384 [==============================] - 0s 56us/step - loss: 0.3784 - accuracy: 0.8620 - val_loss: 0.4132 - val_accuracy: 0.8474\n",
      "Epoch 76/150\n",
      "1384/1384 [==============================] - 0s 58us/step - loss: 0.3725 - accuracy: 0.8663 - val_loss: 0.3956 - val_accuracy: 0.8523\n",
      "Epoch 77/150\n",
      "1384/1384 [==============================] - 0s 68us/step - loss: 0.3769 - accuracy: 0.8692 - val_loss: 0.4251 - val_accuracy: 0.8409\n",
      "Epoch 78/150\n",
      "1384/1384 [==============================] - 0s 86us/step - loss: 0.4048 - accuracy: 0.8555 - val_loss: 0.4105 - val_accuracy: 0.8571\n",
      "Epoch 79/150\n",
      "1384/1384 [==============================] - 0s 93us/step - loss: 0.3707 - accuracy: 0.8613 - val_loss: 0.4048 - val_accuracy: 0.8425\n",
      "Epoch 80/150\n",
      "1384/1384 [==============================] - 0s 103us/step - loss: 0.3781 - accuracy: 0.8540 - val_loss: 0.5787 - val_accuracy: 0.7971\n",
      "Epoch 81/150\n",
      "1384/1384 [==============================] - 0s 89us/step - loss: 0.3841 - accuracy: 0.8620 - val_loss: 0.4111 - val_accuracy: 0.8360\n",
      "Epoch 82/150\n",
      "1384/1384 [==============================] - 0s 73us/step - loss: 0.3684 - accuracy: 0.8678 - val_loss: 0.4522 - val_accuracy: 0.8166\n",
      "Epoch 83/150\n",
      "1384/1384 [==============================] - 0s 61us/step - loss: 0.3720 - accuracy: 0.8685 - val_loss: 0.4095 - val_accuracy: 0.8360\n",
      "Epoch 84/150\n",
      "1384/1384 [==============================] - 0s 63us/step - loss: 0.3744 - accuracy: 0.8634 - val_loss: 0.3874 - val_accuracy: 0.8588\n",
      "Epoch 85/150\n",
      "1384/1384 [==============================] - 0s 63us/step - loss: 0.3743 - accuracy: 0.8663 - val_loss: 0.4970 - val_accuracy: 0.8555\n",
      "Epoch 86/150\n",
      "1384/1384 [==============================] - 0s 63us/step - loss: 0.3907 - accuracy: 0.8671 - val_loss: 0.4900 - val_accuracy: 0.8117\n",
      "Epoch 87/150\n",
      "1384/1384 [==============================] - 0s 83us/step - loss: 0.3641 - accuracy: 0.8591 - val_loss: 0.3930 - val_accuracy: 0.8539\n",
      "Epoch 88/150\n",
      "1384/1384 [==============================] - 0s 68us/step - loss: 0.4380 - accuracy: 0.8598 - val_loss: 0.4137 - val_accuracy: 0.8360\n",
      "Epoch 89/150\n",
      "1384/1384 [==============================] - 0s 69us/step - loss: 0.3736 - accuracy: 0.8678 - val_loss: 0.4325 - val_accuracy: 0.8458\n",
      "Epoch 90/150\n",
      "1384/1384 [==============================] - 0s 62us/step - loss: 0.3723 - accuracy: 0.8649 - val_loss: 0.3979 - val_accuracy: 0.8523\n",
      "Epoch 91/150\n",
      "1384/1384 [==============================] - 0s 62us/step - loss: 0.3590 - accuracy: 0.8663 - val_loss: 0.3991 - val_accuracy: 0.8539\n",
      "Epoch 92/150\n",
      "1384/1384 [==============================] - 0s 71us/step - loss: 0.4329 - accuracy: 0.8598 - val_loss: 0.4053 - val_accuracy: 0.8506\n",
      "Epoch 93/150\n",
      "1384/1384 [==============================] - 0s 61us/step - loss: 0.3654 - accuracy: 0.8721 - val_loss: 0.4972 - val_accuracy: 0.8506\n",
      "Epoch 94/150\n",
      "1384/1384 [==============================] - 0s 59us/step - loss: 0.3778 - accuracy: 0.8627 - val_loss: 0.4067 - val_accuracy: 0.8442\n",
      "Epoch 95/150\n",
      "1384/1384 [==============================] - 0s 67us/step - loss: 0.3722 - accuracy: 0.8598 - val_loss: 0.3929 - val_accuracy: 0.8506\n",
      "Epoch 96/150\n",
      "1384/1384 [==============================] - 0s 69us/step - loss: 0.3694 - accuracy: 0.8671 - val_loss: 0.4399 - val_accuracy: 0.8490\n",
      "Epoch 97/150\n",
      "1384/1384 [==============================] - 0s 78us/step - loss: 0.3699 - accuracy: 0.8678 - val_loss: 0.5881 - val_accuracy: 0.8068\n",
      "Epoch 98/150\n",
      "1384/1384 [==============================] - 0s 71us/step - loss: 0.3552 - accuracy: 0.8728 - val_loss: 0.4304 - val_accuracy: 0.8328\n",
      "Epoch 99/150\n",
      "1384/1384 [==============================] - 0s 68us/step - loss: 0.3659 - accuracy: 0.8656 - val_loss: 0.5577 - val_accuracy: 0.7484\n",
      "Epoch 100/150\n",
      "1384/1384 [==============================] - 0s 63us/step - loss: 0.3690 - accuracy: 0.8678 - val_loss: 0.3781 - val_accuracy: 0.8604\n",
      "Epoch 101/150\n",
      "1384/1384 [==============================] - 0s 58us/step - loss: 0.3624 - accuracy: 0.8678 - val_loss: 0.5335 - val_accuracy: 0.8377\n",
      "Epoch 102/150\n",
      "1384/1384 [==============================] - 0s 65us/step - loss: 0.3886 - accuracy: 0.8627 - val_loss: 0.3784 - val_accuracy: 0.8588\n",
      "Epoch 103/150\n",
      "1384/1384 [==============================] - 0s 61us/step - loss: 0.3904 - accuracy: 0.8663 - val_loss: 0.4269 - val_accuracy: 0.8377\n",
      "Epoch 104/150\n",
      "1384/1384 [==============================] - 0s 58us/step - loss: 0.3585 - accuracy: 0.8663 - val_loss: 0.4026 - val_accuracy: 0.8458\n",
      "Epoch 105/150\n",
      "1384/1384 [==============================] - 0s 56us/step - loss: 0.3800 - accuracy: 0.8692 - val_loss: 0.4033 - val_accuracy: 0.8409\n",
      "Epoch 106/150\n",
      "1384/1384 [==============================] - 0s 55us/step - loss: 0.3587 - accuracy: 0.8678 - val_loss: 0.3907 - val_accuracy: 0.8669\n",
      "Epoch 107/150\n",
      "1384/1384 [==============================] - 0s 55us/step - loss: 0.3565 - accuracy: 0.8692 - val_loss: 0.4364 - val_accuracy: 0.8328\n",
      "Epoch 108/150\n",
      "1384/1384 [==============================] - 0s 59us/step - loss: 0.3662 - accuracy: 0.8692 - val_loss: 0.4000 - val_accuracy: 0.8620\n",
      "Epoch 109/150\n",
      "1384/1384 [==============================] - 0s 69us/step - loss: 0.3671 - accuracy: 0.8671 - val_loss: 0.4782 - val_accuracy: 0.8036\n",
      "Epoch 110/150\n",
      "1384/1384 [==============================] - 0s 68us/step - loss: 0.3704 - accuracy: 0.8692 - val_loss: 0.4387 - val_accuracy: 0.8328\n",
      "Epoch 111/150\n",
      "1384/1384 [==============================] - 0s 55us/step - loss: 0.3733 - accuracy: 0.8620 - val_loss: 0.4154 - val_accuracy: 0.8490\n",
      "Epoch 112/150\n",
      "1384/1384 [==============================] - 0s 67us/step - loss: 0.3686 - accuracy: 0.8707 - val_loss: 0.3921 - val_accuracy: 0.8425\n",
      "Epoch 113/150\n",
      "1384/1384 [==============================] - 0s 81us/step - loss: 0.3613 - accuracy: 0.8620 - val_loss: 0.3963 - val_accuracy: 0.8571\n",
      "Epoch 114/150\n",
      "1384/1384 [==============================] - 0s 78us/step - loss: 0.3744 - accuracy: 0.8584 - val_loss: 0.4202 - val_accuracy: 0.8425\n",
      "Epoch 115/150\n",
      "1384/1384 [==============================] - 0s 84us/step - loss: 0.3576 - accuracy: 0.8671 - val_loss: 0.3964 - val_accuracy: 0.8539\n",
      "Epoch 116/150\n",
      "1384/1384 [==============================] - 0s 95us/step - loss: 0.3574 - accuracy: 0.8634 - val_loss: 0.3867 - val_accuracy: 0.8571\n",
      "Epoch 117/150\n",
      "1384/1384 [==============================] - 0s 76us/step - loss: 0.3531 - accuracy: 0.8714 - val_loss: 0.4300 - val_accuracy: 0.8458\n",
      "Epoch 118/150\n",
      "1384/1384 [==============================] - 0s 84us/step - loss: 0.3706 - accuracy: 0.8598 - val_loss: 0.4445 - val_accuracy: 0.8474\n",
      "Epoch 119/150\n",
      "1384/1384 [==============================] - 0s 88us/step - loss: 0.3609 - accuracy: 0.8613 - val_loss: 0.3907 - val_accuracy: 0.8490\n",
      "Epoch 120/150\n",
      "1384/1384 [==============================] - 0s 83us/step - loss: 0.3912 - accuracy: 0.8642 - val_loss: 0.4057 - val_accuracy: 0.8539\n",
      "Epoch 121/150\n",
      "1384/1384 [==============================] - 0s 69us/step - loss: 0.3518 - accuracy: 0.8707 - val_loss: 0.3919 - val_accuracy: 0.8571\n",
      "Epoch 122/150\n",
      "1384/1384 [==============================] - 0s 67us/step - loss: 0.3675 - accuracy: 0.8743 - val_loss: 0.4260 - val_accuracy: 0.8490\n",
      "Epoch 123/150\n",
      "1384/1384 [==============================] - 0s 75us/step - loss: 0.3459 - accuracy: 0.8779 - val_loss: 0.4452 - val_accuracy: 0.8295\n",
      "Epoch 124/150\n",
      "1384/1384 [==============================] - 0s 68us/step - loss: 0.3595 - accuracy: 0.8649 - val_loss: 0.3779 - val_accuracy: 0.8636\n",
      "Epoch 125/150\n",
      "1384/1384 [==============================] - 0s 73us/step - loss: 0.3644 - accuracy: 0.8671 - val_loss: 0.4034 - val_accuracy: 0.8409\n",
      "Epoch 126/150\n",
      "1384/1384 [==============================] - 0s 76us/step - loss: 0.3542 - accuracy: 0.8656 - val_loss: 0.4437 - val_accuracy: 0.8328\n",
      "Epoch 127/150\n",
      "1384/1384 [==============================] - 0s 82us/step - loss: 0.3667 - accuracy: 0.8598 - val_loss: 0.4116 - val_accuracy: 0.8442\n",
      "Epoch 128/150\n",
      "1384/1384 [==============================] - 0s 77us/step - loss: 0.3676 - accuracy: 0.8671 - val_loss: 0.3976 - val_accuracy: 0.8571\n",
      "Epoch 129/150\n",
      "1384/1384 [==============================] - 0s 86us/step - loss: 0.3548 - accuracy: 0.8678 - val_loss: 0.4205 - val_accuracy: 0.8490\n",
      "Epoch 130/150\n",
      "1384/1384 [==============================] - 0s 95us/step - loss: 0.3718 - accuracy: 0.8634 - val_loss: 0.3898 - val_accuracy: 0.8766\n",
      "Epoch 131/150\n",
      "1384/1384 [==============================] - 0s 102us/step - loss: 0.3559 - accuracy: 0.8699 - val_loss: 0.4014 - val_accuracy: 0.8669\n",
      "Epoch 132/150\n",
      "1384/1384 [==============================] - 0s 77us/step - loss: 0.3565 - accuracy: 0.8699 - val_loss: 0.4207 - val_accuracy: 0.8409\n",
      "Epoch 133/150\n",
      "1384/1384 [==============================] - 0s 87us/step - loss: 0.3634 - accuracy: 0.8692 - val_loss: 0.5425 - val_accuracy: 0.7500\n",
      "Epoch 134/150\n",
      "1384/1384 [==============================] - 0s 89us/step - loss: 0.3713 - accuracy: 0.8642 - val_loss: 0.5041 - val_accuracy: 0.8263\n",
      "Epoch 135/150\n",
      "1384/1384 [==============================] - 0s 71us/step - loss: 0.3717 - accuracy: 0.8584 - val_loss: 0.3873 - val_accuracy: 0.8539\n",
      "Epoch 136/150\n",
      "1384/1384 [==============================] - 0s 90us/step - loss: 0.3659 - accuracy: 0.8627 - val_loss: 0.5610 - val_accuracy: 0.8409\n",
      "Epoch 137/150\n",
      "1384/1384 [==============================] - 0s 87us/step - loss: 0.3695 - accuracy: 0.8605 - val_loss: 0.4068 - val_accuracy: 0.8474\n",
      "Epoch 138/150\n",
      "1384/1384 [==============================] - 0s 70us/step - loss: 0.3513 - accuracy: 0.8692 - val_loss: 0.4447 - val_accuracy: 0.8312\n",
      "Epoch 139/150\n",
      "1384/1384 [==============================] - 0s 75us/step - loss: 0.3679 - accuracy: 0.8649 - val_loss: 0.3919 - val_accuracy: 0.8571\n",
      "Epoch 140/150\n",
      "1384/1384 [==============================] - 0s 89us/step - loss: 0.3569 - accuracy: 0.8671 - val_loss: 0.3763 - val_accuracy: 0.8701\n",
      "Epoch 141/150\n",
      "1384/1384 [==============================] - 0s 91us/step - loss: 0.3576 - accuracy: 0.8591 - val_loss: 0.3899 - val_accuracy: 0.8523\n",
      "Epoch 142/150\n",
      "1384/1384 [==============================] - 0s 83us/step - loss: 0.3765 - accuracy: 0.8577 - val_loss: 0.3974 - val_accuracy: 0.8474\n",
      "Epoch 143/150\n",
      "1384/1384 [==============================] - 0s 86us/step - loss: 0.3474 - accuracy: 0.8663 - val_loss: 0.3867 - val_accuracy: 0.8718\n",
      "Epoch 144/150\n",
      "1384/1384 [==============================] - 0s 93us/step - loss: 0.3493 - accuracy: 0.8728 - val_loss: 0.4106 - val_accuracy: 0.8523\n",
      "Epoch 145/150\n",
      "1384/1384 [==============================] - 0s 90us/step - loss: 0.3509 - accuracy: 0.8736 - val_loss: 0.3970 - val_accuracy: 0.8490\n",
      "Epoch 146/150\n",
      "1384/1384 [==============================] - 0s 89us/step - loss: 0.3571 - accuracy: 0.8663 - val_loss: 0.3799 - val_accuracy: 0.8523\n",
      "Epoch 147/150\n",
      "1384/1384 [==============================] - 0s 106us/step - loss: 0.3526 - accuracy: 0.8707 - val_loss: 0.4399 - val_accuracy: 0.8474\n",
      "Epoch 148/150\n",
      "1384/1384 [==============================] - 0s 92us/step - loss: 0.3682 - accuracy: 0.8649 - val_loss: 0.3852 - val_accuracy: 0.8458\n",
      "Epoch 149/150\n",
      "1384/1384 [==============================] - 0s 79us/step - loss: 0.3692 - accuracy: 0.8714 - val_loss: 0.4032 - val_accuracy: 0.8409\n",
      "Epoch 150/150\n",
      "1384/1384 [==============================] - 0s 80us/step - loss: 0.3647 - accuracy: 0.8707 - val_loss: 0.4174 - val_accuracy: 0.8766\n",
      "the length of the true_class is 616\n",
      "the lenth of the pred class is 616\n",
      "the lenth of the pred-prob class is 616\n",
      "\n",
      "\n",
      "==========================================================\n",
      "basic model: 1384 samples\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8691    0.8566    0.8628       279\n",
      "           1     0.8827    0.8932    0.8879       337\n",
      "\n",
      "    accuracy                         0.8766       616\n",
      "   macro avg     0.8759    0.8749    0.8754       616\n",
      "weighted avg     0.8765    0.8766    0.8765       616\n",
      "\n",
      "recall-None [0.85663082 0.89317507]\n",
      "recall-macro  0.874902949278368\n",
      "recall-micro  0.8766233766233766\n",
      "[0.42347547760257476, 0.8766233921051025]\n",
      "loss: 0.42347547760257476\n",
      "accuracy: 0.8766233921051025\n"
     ]
    }
   ],
   "source": [
    "asl = ASL(1384,1384)\n",
    "asl.simple_setup()\n",
    "y_t,y_p,probs=asl.train_basic_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
